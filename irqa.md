### QA Intro
- Martin & Jurafsky, 3era ed. Capítulo 23: Question Answering [Link](https://web.stanford.edu/~jurafsky/slp3/23.pdf)


### Papers sobre Information Retrieval en el marco de QA

- Moldovan, D., & Surdeanu, M. (2002, July). On the role of information retrieval and information extraction in question answering systems. 
In _International Summer School on Information Extraction_ (pp. 129-147). Springer, Berlin, Heidelberg. [Link](https://link.springer.com/chapter/10.1007/978-3-540-45092-4_6)

- Libro de IR: Manning, C. D., Raghavan, P., and Schu ̈tze, H. (2008). Introduction to Information Retrieval. Cambridge.[Link](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf)

- Karpukhin, V., Og ̆uz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. (2020). Dense passage retrieval for open-domain question answering. EMNLP. [Link](https://arxiv.org/pdf/2004.04906.pdf)

**Abstract** _Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene- BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks_. 

**TLDR** En este paper se realiza la etapa de IR con un modelo llamado Dense Passage Retriever (DPR), el cual utiliza BERT para los encoders y el algoritmo FAISS para la inferencia. Se agregan en el entrenamiento ejemplos negativos. Los resultados comparados con el estado del arte hasta el 2019 son mejores, comparando con BM25 y también con otros modelos de embeddings. Indica que el tiempo de indexación de FAISS (8,5 hs para 21millones de pasajes) es mucho mayor que el que requiere BM25 (30 min). Una vez indexado, la ejecución es más rápida para FAISS. Además al tiempo de indexación es necesario sumarle el de calcular los embeddings (8,8hs).


- Chen, D., Fisch, A., Weston, J., and Bordes, A. (2017). Reading wikipedia to answer open-domain questions. ACL. [Link](https://arxiv.org/pdf/1704.00051.pdf)

**Abstract** _This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task_.

**TLDR** En este paper se realiza la etapa de IR con un modelo basado en TF-IDF utilizando una estructura de Hash con bigramas, en lugar de términos. Los resultados mejoran el buscador de wikipedia (implementado en ese momento con ElasticSearch). 
En los experimentos este método resultó mejor que TF-IDF con unigramas, o BM25, y también pasando preguntas y documentos a embeddings y aplicando distancia coseno. Este último ejemplo no aparece en la tabla de comparación de los resultados. 
La evaluación de los métodos lo realiza considerando la proporción de preguntas para las que el texto de cualquiera de sus respuestas asociadas aparece en al menos una de las 5 páginas más relevantes devueltas por cada sistema.


- Lin, J., Nogueira, R., and Yates, A. (2020). Pretrained transformers for text ranking: BERT and beyond. arXiv preprint arXiv:2010.06467. [Link](https://arxiv.org/pdf/2010.06467.pdf) 

**Abstract** _The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This survey provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has, without exaggeration, revolutionized the fields of natural language processing (NLP), information retrieval (IR), and beyond. In the context of text ranking, these models produce high quality results across many domains, tasks, and settings.
In this survey, we provide a synthesis of existing work as a single point of entry for practitioners who wish to gain a better understanding of how to apply transformers to text ranking problems and researchers who wish to pursue work in this area. We cover a wide range of modern techniques, grouped into two high-level categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that attempt to perform ranking directly. There are numerous examples that fall into the first category, including approaches based on relevance classification, evidence aggregation from multiple segments of text, corpus analysis, and sequence-to-sequence models. While the second category of approaches is less well studied, representation learning with transformers is an emerging and exciting direction that is bound to attract more attention moving forward. There are two themes that pervade our survey: techniques for handling long documents, beyond the typical sentence-by-sentence processing approaches used in NLP, and techniques for addressing the tradeoff between effectiveness (result quality) and efficiency (query latency).
Although transformer architectures and pretraining techniques are recent innovations, many aspects of how they are applied to text ranking are relatively well understood and represent mature techniques. However, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, this survey also attempts to prognosticate where the field is heading_.

**TLDR** Así como su abstract, el documento es largo (130 páginas, no tiene estructura de paper). La idea del documento es presentar el estado del arte de “Text Ranking” o IR. Según la introducción, esta tarea al día de hoy se realiza con BERT, y los grandes motores de búsqueda lo utilizan para la tarea de IR. 
En la introducción hay una sección de problemas de IR. en particular uno de los problemas es la necesidad de exactitud de términos para el IR sin embeddings. Como solución se propone en primer lugar la expansión de consultas (la idea que teníamos para investigar). De ahí hace referencia al siguiente paper (Query Expansion Using Lexical-Semantic Relations.1995)

- M. Voorhees. Query expansion using lexical-semantic relations. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1994), pages 61–69, Dublin, Ireland, 1994.[Link](https://www.researchgate.net/publication/221300122_Query_Expansion_Using_Lexical-Semantic_Relations)

**Abstract** _Applications such as office automation, news filtering, help facilities in complex systems, and the like require the ability to retrieve documents from full-text databases where vocabulary problems can be particularly severe. Experiments performed on small collections with single-domain thesauri suggest that expanding query vectors with words that are lexically related to the original query words can ameliorate some of the problems of mismatched vocabularies. This paper examines the utility of lexical query expansion in the large, diverse TREC collection. Concepts are represented by WordNet synonym sets and are expanded by following the typed links included in WordNet. Experimental results show this query expansion technique makes little difference in retrieval effectiveness if the original queries are relatively complete descriptions of the information being sought even when the concepts to be expanded are selected by hand. Less well developed queries can be significantly improved by expansion of hand-chosen concepts. However, an automatic procedure that can approximate the set of hand picked synonym sets has yet to be devised, and expanding by the synonym sets that are automatically generated can degrade retrieval performance_.

**TLDR** En este trabajo se expanden consultas agregándole a la consulta términos relacionados de forma léxica-semántica, utilizando wordNet. Los resultados no aportan beneficios significativos. 

- Kamphuis, C., de Vries, A. P., Boytsov, L., and Lin, J. (2020). Which bm25 do you mean? a large-scale repro- ducibility study of scoring variants. European Conference on Information Retrieval. [Link](https://link.springer.com/chapter/10.1007/978-3-030-45442-5_4) 

**Abstract** _When researchers speak of BM25, it is not entirely clear which variant they mean, since many tweaks to Robertson et al.’s original formulation have been proposed. When practitioners speak of BM25, they most likely refer to the implementation in the Lucene open-source search library. Does this ambiguity “matter”? We attempt to answer this question with a large-scale reproducibility study of BM25, considering eight variants. Experiments on three newswire collections show that there are no significant effectiveness differences between them, including Lucene’s often maligned approximation of document length. As an added benefit, our empirical approach takes advantage of databases for rapid IR prototyping, which validates both the feasibility and methodological advantages claimed in previous work_.
